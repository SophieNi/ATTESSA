{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Model output directory: ../models *****\n"
     ]
    }
   ],
   "source": [
    "# Set the output directory for saving model file\n",
    "# Optionally, set a GCP bucket location\n",
    "\n",
    "OUTPUT_DIR = '../models'\n",
    "DO_DELETE = False \n",
    "USE_BUCKET = False \n",
    "BUCKET = 'BUCKET_NAME'\n",
    "\n",
    "if USE_BUCKET:\n",
    "    OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "\n",
    "if DO_DELETE:\n",
    "    try:\n",
    "        tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "    data = {}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"sentiment\"] = []\n",
    "    \n",
    "    for file_path in os.listdir(directory):\n",
    "        with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "            data[\"sentence\"].append(f.read())\n",
    "            data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "            \n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "    pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "    neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "    pos_df[\"polarity\"] = 1\n",
    "    neg_df[\"polarity\"] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_dataset(os.path.join(\"../data/\", \"aclImdb\", \"train\"))\n",
    "test = load_dataset(os.path.join(\"../data/\", \"aclImdb\", \"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sample(5000)\n",
    "test = test.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'sentence'\n",
    "LABEL_COLUMN = 'polarity'\n",
    "label_list = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None,\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tingwei758/opt/anaconda3/envs/ATTESSA/lib/python3.7/site-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tingwei758/opt/anaconda3/envs/ATTESSA/lib/python3.7/site-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "    return bert.tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'here',\n",
       " \"'\",\n",
       " 's',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'using',\n",
       " 'the',\n",
       " 'bert',\n",
       " 'token',\n",
       " '##izer']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tingwei758/opt/anaconda3/envs/ATTESSA/lib/python3.7/site-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tingwei758/opt/anaconda3/envs/ATTESSA/lib/python3.7/site-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] all i can really say is that i ' m glad that i was knitting socks while watching the movie , or i would be very angry for having wasted 2 hours of my life . the acting was terrible , the plot was even worse . there were some scenes that were meant to be serious that had my husband and i laughing out loud . i highly recommend this movie to people who like to do their own version of ms ##t ##3 ##k . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] all i can really say is that i ' m glad that i was knitting socks while watching the movie , or i would be very angry for having wasted 2 hours of my life . the acting was terrible , the plot was even worse . there were some scenes that were meant to be serious that had my husband and i laughing out loud . i highly recommend this movie to people who like to do their own version of ms ##t ##3 ##k . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2035 1045 2064 2428 2360 2003 2008 1045 1005 1049 5580 2008 1045 2001 26098 14829 2096 3666 1996 3185 1010 2030 1045 2052 2022 2200 4854 2005 2383 13842 1016 2847 1997 2026 2166 1012 1996 3772 2001 6659 1010 1996 5436 2001 2130 4788 1012 2045 2020 2070 5019 2008 2020 3214 2000 2022 3809 2008 2018 2026 3129 1998 1045 5870 2041 5189 1012 1045 3811 16755 2023 3185 2000 2111 2040 2066 2000 2079 2037 2219 2544 1997 5796 2102 2509 2243 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2035 1045 2064 2428 2360 2003 2008 1045 1005 1049 5580 2008 1045 2001 26098 14829 2096 3666 1996 3185 1010 2030 1045 2052 2022 2200 4854 2005 2383 13842 1016 2847 1997 2026 2166 1012 1996 3772 2001 6659 1010 1996 5436 2001 2130 4788 1012 2045 2020 2070 5019 2008 2020 3214 2000 2022 3809 2008 2018 2026 3129 1998 1045 5870 2041 5189 1012 1045 3811 16755 2023 3185 2000 2111 2040 2066 2000 2079 2037 2219 2544 1997 5796 2102 2509 2243 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i will say that at least the movie makes sense , but it ' s bad . the acting for the most part is not good ( i think only sky showed any promise ) and you feel awkward watching it . all of the scenes that should be meaningful are really shallow , like when ng comes out to her parents . there are a lot of corn ##y details , like the kan ##ji tattoo on the sky ' s shoulder , the magnet ##s on the girls ' refrigerator and the god awful decor at the sets . . . and the music . . . and clothes . . and everything . real life has never been like this movie . the [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i will say that at least the movie makes sense , but it ' s bad . the acting for the most part is not good ( i think only sky showed any promise ) and you feel awkward watching it . all of the scenes that should be meaningful are really shallow , like when ng comes out to her parents . there are a lot of corn ##y details , like the kan ##ji tattoo on the sky ' s shoulder , the magnet ##s on the girls ' refrigerator and the god awful decor at the sets . . . and the music . . . and clothes . . and everything . real life has never been like this movie . the [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 2097 2360 2008 2012 2560 1996 3185 3084 3168 1010 2021 2009 1005 1055 2919 1012 1996 3772 2005 1996 2087 2112 2003 2025 2204 1006 1045 2228 2069 3712 3662 2151 4872 1007 1998 2017 2514 9596 3666 2009 1012 2035 1997 1996 5019 2008 2323 2022 15902 2024 2428 8467 1010 2066 2043 12835 3310 2041 2000 2014 3008 1012 2045 2024 1037 2843 1997 9781 2100 4751 1010 2066 1996 22827 4478 11660 2006 1996 3712 1005 1055 3244 1010 1996 16853 2015 2006 1996 3057 1005 18097 1998 1996 2643 9643 25545 2012 1996 4520 1012 1012 1012 1998 1996 2189 1012 1012 1012 1998 4253 1012 1012 1998 2673 1012 2613 2166 2038 2196 2042 2066 2023 3185 1012 1996 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 2097 2360 2008 2012 2560 1996 3185 3084 3168 1010 2021 2009 1005 1055 2919 1012 1996 3772 2005 1996 2087 2112 2003 2025 2204 1006 1045 2228 2069 3712 3662 2151 4872 1007 1998 2017 2514 9596 3666 2009 1012 2035 1997 1996 5019 2008 2323 2022 15902 2024 2428 8467 1010 2066 2043 12835 3310 2041 2000 2014 3008 1012 2045 2024 1037 2843 1997 9781 2100 4751 1010 2066 1996 22827 4478 11660 2006 1996 3712 1005 1055 3244 1010 1996 16853 2015 2006 1996 3057 1005 18097 1998 1996 2643 9643 25545 2012 1996 4520 1012 1012 1012 1998 1996 2189 1012 1012 1012 1998 4253 1012 1012 1998 2673 1012 2613 2166 2038 2196 2042 2066 2023 3185 1012 1996 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] brief encounter is a g ##has ##tly and pointless remake of the 1945 david lean classic , which was based on noel coward ' s play \" still life \" . a doctor removes a particle of grit from a woman ' s eye at a railway station , he is in a miserable relationship , she is happily married social worker of italian ancestry . they meet by accident on another occasion , form an instant attraction and arrange to meet each other every wednesday . the pair fall in love , but after spending a few afternoons together they realise that they have no realistic chance of happiness and agree to part . coward ' s original one - act play concerned two ordinary [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] brief encounter is a g ##has ##tly and pointless remake of the 1945 david lean classic , which was based on noel coward ' s play \" still life \" . a doctor removes a particle of grit from a woman ' s eye at a railway station , he is in a miserable relationship , she is happily married social worker of italian ancestry . they meet by accident on another occasion , form an instant attraction and arrange to meet each other every wednesday . the pair fall in love , but after spending a few afternoons together they realise that they have no realistic chance of happiness and agree to part . coward ' s original one - act play concerned two ordinary [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 4766 8087 2003 1037 1043 14949 14626 1998 23100 12661 1997 1996 3386 2585 8155 4438 1010 2029 2001 2241 2006 10716 16592 1005 1055 2377 1000 2145 2166 1000 1012 1037 3460 20362 1037 10811 1997 24842 2013 1037 2450 1005 1055 3239 2012 1037 2737 2276 1010 2002 2003 1999 1037 13736 3276 1010 2016 2003 11361 2496 2591 7309 1997 3059 11377 1012 2027 3113 2011 4926 2006 2178 6686 1010 2433 2019 7107 8432 1998 13621 2000 3113 2169 2060 2296 9317 1012 1996 3940 2991 1999 2293 1010 2021 2044 5938 1037 2261 24738 2362 2027 19148 2008 2027 2031 2053 12689 3382 1997 8404 1998 5993 2000 2112 1012 16592 1005 1055 2434 2028 1011 2552 2377 4986 2048 6623 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 4766 8087 2003 1037 1043 14949 14626 1998 23100 12661 1997 1996 3386 2585 8155 4438 1010 2029 2001 2241 2006 10716 16592 1005 1055 2377 1000 2145 2166 1000 1012 1037 3460 20362 1037 10811 1997 24842 2013 1037 2450 1005 1055 3239 2012 1037 2737 2276 1010 2002 2003 1999 1037 13736 3276 1010 2016 2003 11361 2496 2591 7309 1997 3059 11377 1012 2027 3113 2011 4926 2006 2178 6686 1010 2433 2019 7107 8432 1998 13621 2000 3113 2169 2060 2296 9317 1012 1996 3940 2991 1999 2293 1010 2021 2044 5938 1037 2261 24738 2362 2027 19148 2008 2027 2031 2053 12689 3382 1997 8404 1998 5993 2000 2112 1012 16592 1005 1055 2434 2028 1011 2552 2377 4986 2048 6623 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] * * * * * * * * spoil ##ers - - careful * * * * * * * * * < br / > < br / > what can i say ? i ' m biased when it comes to urban cowboy . i love it and have watched it countless times - - and usually find out something new about it with each viewing . < br / > < br / > i think one of the things i like about it is that urban cowboy is about working class people , not rich people who live in either l . a . or new york . well , it is true except for pam . < br / > < [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] * * * * * * * * spoil ##ers - - careful * * * * * * * * * < br / > < br / > what can i say ? i ' m biased when it comes to urban cowboy . i love it and have watched it countless times - - and usually find out something new about it with each viewing . < br / > < br / > i think one of the things i like about it is that urban cowboy is about working class people , not rich people who live in either l . a . or new york . well , it is true except for pam . < br / > < [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1008 1008 1008 1008 1008 1008 1008 1008 27594 2545 1011 1011 6176 1008 1008 1008 1008 1008 1008 1008 1008 1008 1026 7987 1013 1028 1026 7987 1013 1028 2054 2064 1045 2360 1029 1045 1005 1049 25352 2043 2009 3310 2000 3923 11762 1012 1045 2293 2009 1998 2031 3427 2009 14518 2335 1011 1011 1998 2788 2424 2041 2242 2047 2055 2009 2007 2169 10523 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 2228 2028 1997 1996 2477 1045 2066 2055 2009 2003 2008 3923 11762 2003 2055 2551 2465 2111 1010 2025 4138 2111 2040 2444 1999 2593 1048 1012 1037 1012 2030 2047 2259 1012 2092 1010 2009 2003 2995 3272 2005 14089 1012 1026 7987 1013 1028 1026 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1008 1008 1008 1008 1008 1008 1008 1008 27594 2545 1011 1011 6176 1008 1008 1008 1008 1008 1008 1008 1008 1008 1026 7987 1013 1028 1026 7987 1013 1028 2054 2064 1045 2360 1029 1045 1005 1049 25352 2043 2009 3310 2000 3923 11762 1012 1045 2293 2009 1998 2031 3427 2009 14518 2335 1011 1011 1998 2788 2424 2041 2242 2047 2055 2009 2007 2169 10523 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 2228 2028 1997 1996 2477 1045 2066 2055 2009 2003 2008 3923 11762 2003 2055 2551 2465 2111 1010 2025 4138 2111 2040 2444 1999 2593 1048 1012 1037 1012 2030 2047 2259 1012 2092 1010 2009 2003 2995 3272 2005 14089 1012 1026 7987 1013 1028 1026 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] why else would he do this to me ? < br / > < br / > not that i expect dean cain to produce hit movies . or even decent movies . i saw lois and clark , i am aware of just how . . . \" good \" dean cain is . < br / > < br / > obviously this is gonna be a cheese ##y flick , and each cheese ##y flick has its own special way to make you scratch your head . i will not call these spoil ##ers as you can ' t really spoil this movie any more than it already is . < br / > < br / > to begin with . . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] why else would he do this to me ? < br / > < br / > not that i expect dean cain to produce hit movies . or even decent movies . i saw lois and clark , i am aware of just how . . . \" good \" dean cain is . < br / > < br / > obviously this is gonna be a cheese ##y flick , and each cheese ##y flick has its own special way to make you scratch your head . i will not call these spoil ##ers as you can ' t really spoil this movie any more than it already is . < br / > < br / > to begin with . . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2339 2842 2052 2002 2079 2023 2000 2033 1029 1026 7987 1013 1028 1026 7987 1013 1028 2025 2008 1045 5987 4670 11557 2000 3965 2718 5691 1012 2030 2130 11519 5691 1012 1045 2387 15815 1998 5215 1010 1045 2572 5204 1997 2074 2129 1012 1012 1012 1000 2204 1000 4670 11557 2003 1012 1026 7987 1013 1028 1026 7987 1013 1028 5525 2023 2003 6069 2022 1037 8808 2100 17312 1010 1998 2169 8808 2100 17312 2038 2049 2219 2569 2126 2000 2191 2017 11969 2115 2132 1012 1045 2097 2025 2655 2122 27594 2545 2004 2017 2064 1005 1056 2428 27594 2023 3185 2151 2062 2084 2009 2525 2003 1012 1026 7987 1013 1028 1026 7987 1013 1028 2000 4088 2007 1012 1012 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2339 2842 2052 2002 2079 2023 2000 2033 1029 1026 7987 1013 1028 1026 7987 1013 1028 2025 2008 1045 5987 4670 11557 2000 3965 2718 5691 1012 2030 2130 11519 5691 1012 1045 2387 15815 1998 5215 1010 1045 2572 5204 1997 2074 2129 1012 1012 1012 1000 2204 1000 4670 11557 2003 1012 1026 7987 1013 1028 1026 7987 1013 1028 5525 2023 2003 6069 2022 1037 8808 2100 17312 1010 1998 2169 8808 2100 17312 2038 2049 2219 2569 2126 2000 2191 2017 11969 2115 2132 1012 1045 2097 2025 2655 2122 27594 2545 2004 2017 2064 1005 1056 2428 27594 2023 3185 2151 2062 2084 2009 2525 2003 1012 1026 7987 1013 1028 1026 7987 1013 1028 2000 4088 2007 1012 1012 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] wow , what exciting visual effects . i also loved the costumes and artwork , the circus and ether ##eal feel to the film was sublime . it just required the need for the viewer to worry about the fate of our protagonist . as she is trapped in her imagination , there is never a sense of per ##il unlike , say , david lynch ' s films which haunt every time . this also draws attention to which age group this film is aimed at . who would this engage ? < br / > < br / > mirror ##mas ##k is obviously going to draw comparisons with labyrinth with the teen - ang ##st / fantasy theme , but unfortunately it doesn [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] wow , what exciting visual effects . i also loved the costumes and artwork , the circus and ether ##eal feel to the film was sublime . it just required the need for the viewer to worry about the fate of our protagonist . as she is trapped in her imagination , there is never a sense of per ##il unlike , say , david lynch ' s films which haunt every time . this also draws attention to which age group this film is aimed at . who would this engage ? < br / > < br / > mirror ##mas ##k is obviously going to draw comparisons with labyrinth with the teen - ang ##st / fantasy theme , but unfortunately it doesn [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 10166 1010 2054 10990 5107 3896 1012 1045 2036 3866 1996 12703 1998 8266 1010 1996 9661 1998 28855 15879 2514 2000 1996 2143 2001 28341 1012 2009 2074 3223 1996 2342 2005 1996 13972 2000 4737 2055 1996 6580 1997 2256 10191 1012 2004 2016 2003 7567 1999 2014 9647 1010 2045 2003 2196 1037 3168 1997 2566 4014 4406 1010 2360 1010 2585 11404 1005 1055 3152 2029 24542 2296 2051 1012 2023 2036 9891 3086 2000 2029 2287 2177 2023 2143 2003 6461 2012 1012 2040 2052 2023 8526 1029 1026 7987 1013 1028 1026 7987 1013 1028 5259 9335 2243 2003 5525 2183 2000 4009 18539 2007 24239 2007 1996 9458 1011 17076 3367 1013 5913 4323 1010 2021 6854 2009 2987 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 10166 1010 2054 10990 5107 3896 1012 1045 2036 3866 1996 12703 1998 8266 1010 1996 9661 1998 28855 15879 2514 2000 1996 2143 2001 28341 1012 2009 2074 3223 1996 2342 2005 1996 13972 2000 4737 2055 1996 6580 1997 2256 10191 1012 2004 2016 2003 7567 1999 2014 9647 1010 2045 2003 2196 1037 3168 1997 2566 4014 4406 1010 2360 1010 2585 11404 1005 1055 3152 2029 24542 2296 2051 1012 2023 2036 9891 3086 2000 2029 2287 2177 2023 2143 2003 6461 2012 1012 2040 2052 2023 8526 1029 1026 7987 1013 1028 1026 7987 1013 1028 5259 9335 2243 2003 5525 2183 2000 4009 18539 2007 24239 2007 1996 9458 1011 17076 3367 1013 5913 4323 1010 2021 6854 2009 2987 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] a truly remarkable film that takes you on a journey through your hidden emotions , a deep and en ##light ##ening story . the story takes you through the lives and beliefs of gay and religious cultures , with excellent performances from an superb star cast . it will touch the deepest reaches of your emotions , for those who belief that a love that risks nothing , is worth nothing . the film starts with introducing us to the characters that on the surface seem to have everything , later we find that beneath the surfaces of each character they are looking for a journey of self discovery . we learn from this story that we all must chose the path in life that we [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] a truly remarkable film that takes you on a journey through your hidden emotions , a deep and en ##light ##ening story . the story takes you through the lives and beliefs of gay and religious cultures , with excellent performances from an superb star cast . it will touch the deepest reaches of your emotions , for those who belief that a love that risks nothing , is worth nothing . the film starts with introducing us to the characters that on the surface seem to have everything , later we find that beneath the surfaces of each character they are looking for a journey of self discovery . we learn from this story that we all must chose the path in life that we [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1037 5621 9487 2143 2008 3138 2017 2006 1037 4990 2083 2115 5023 6699 1010 1037 2784 1998 4372 7138 7406 2466 1012 1996 2466 3138 2017 2083 1996 3268 1998 9029 1997 5637 1998 3412 8578 1010 2007 6581 4616 2013 2019 21688 2732 3459 1012 2009 2097 3543 1996 17578 6561 1997 2115 6699 1010 2005 2216 2040 6772 2008 1037 2293 2008 10831 2498 1010 2003 4276 2498 1012 1996 2143 4627 2007 10449 2149 2000 1996 3494 2008 2006 1996 3302 4025 2000 2031 2673 1010 2101 2057 2424 2008 4218 1996 9972 1997 2169 2839 2027 2024 2559 2005 1037 4990 1997 2969 5456 1012 2057 4553 2013 2023 2466 2008 2057 2035 2442 4900 1996 4130 1999 2166 2008 2057 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1037 5621 9487 2143 2008 3138 2017 2006 1037 4990 2083 2115 5023 6699 1010 1037 2784 1998 4372 7138 7406 2466 1012 1996 2466 3138 2017 2083 1996 3268 1998 9029 1997 5637 1998 3412 8578 1010 2007 6581 4616 2013 2019 21688 2732 3459 1012 2009 2097 3543 1996 17578 6561 1997 2115 6699 1010 2005 2216 2040 6772 2008 1037 2293 2008 10831 2498 1010 2003 4276 2498 1012 1996 2143 4627 2007 10449 2149 2000 1996 3494 2008 2006 1996 3302 4025 2000 2031 2673 1010 2101 2057 2424 2008 4218 1996 9972 1997 2169 2839 2027 2024 2559 2005 1037 4990 1997 2969 5456 1012 2057 4553 2013 2023 2466 2008 2057 2035 2442 4900 1996 4130 1999 2166 2008 2057 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] a touching movie . it is full of emotions and wonderful acting . i could have sat through it a second time . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] a touching movie . it is full of emotions and wonderful acting . i could have sat through it a second time . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1037 7244 3185 1012 2009 2003 2440 1997 6699 1998 6919 3772 1012 1045 2071 2031 2938 2083 2009 1037 2117 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1037 7244 3185 1012 2009 2003 2440 1997 6699 1998 6919 3772 1012 1045 2071 2031 2938 2083 2009 1037 2117 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this has got to be one of absolute worst movies i ' ve ever seen in my life . the writing and acting are just pathetic . it ranks right up there with uncle sam on the all time worst movies ever made . however , when i see crap like this able to make it to video , it really inspire ##s me to pursue my wild dreams of making films because i know i could do a better job than what the makers of killers did . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this has got to be one of absolute worst movies i ' ve ever seen in my life . the writing and acting are just pathetic . it ranks right up there with uncle sam on the all time worst movies ever made . however , when i see crap like this able to make it to video , it really inspire ##s me to pursue my wild dreams of making films because i know i could do a better job than what the makers of killers did . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 2038 2288 2000 2022 2028 1997 7619 5409 5691 1045 1005 2310 2412 2464 1999 2026 2166 1012 1996 3015 1998 3772 2024 2074 17203 1012 2009 6938 2157 2039 2045 2007 4470 3520 2006 1996 2035 2051 5409 5691 2412 2081 1012 2174 1010 2043 1045 2156 10231 2066 2023 2583 2000 2191 2009 2000 2678 1010 2009 2428 18708 2015 2033 2000 7323 2026 3748 5544 1997 2437 3152 2138 1045 2113 1045 2071 2079 1037 2488 3105 2084 2054 1996 11153 1997 15978 2106 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 2038 2288 2000 2022 2028 1997 7619 5409 5691 1045 1005 2310 2412 2464 1999 2026 2166 1012 1996 3015 1998 3772 2024 2074 17203 1012 2009 6938 2157 2039 2045 2007 4470 3520 2006 1996 2035 2051 5409 5691 2412 2081 1012 2174 1010 2043 1045 2156 10231 2066 2023 2583 2000 2191 2009 2000 2678 1010 2009 2428 18708 2015 2033 2000 7323 2026 3748 5544 1997 2437 3152 2138 1045 2113 1045 2071 2079 1037 2488 3105 2084 2054 1996 11153 1997 15978 2106 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] hi guys , this is my first review and i would had to have picked the worst movie to review . as i only watched 5 minutes of it but trust me you could see this movie was going nowhere . the acting was de ##pl ##ora ##ble , the camera work and lighting looked as though it was shot and run by a pack of 10 year old ' s . no offence to 10 year olds . i just couldn ' t take anymore i got off my couch , took the the house of adam dvd out of the dvd player and threw it in the garbage . maybe if you are a colin farrell fan this movie may interest you , because [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] hi guys , this is my first review and i would had to have picked the worst movie to review . as i only watched 5 minutes of it but trust me you could see this movie was going nowhere . the acting was de ##pl ##ora ##ble , the camera work and lighting looked as though it was shot and run by a pack of 10 year old ' s . no offence to 10 year olds . i just couldn ' t take anymore i got off my couch , took the the house of adam dvd out of the dvd player and threw it in the garbage . maybe if you are a colin farrell fan this movie may interest you , because [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 7632 4364 1010 2023 2003 2026 2034 3319 1998 1045 2052 2018 2000 2031 3856 1996 5409 3185 2000 3319 1012 2004 1045 2069 3427 1019 2781 1997 2009 2021 3404 2033 2017 2071 2156 2023 3185 2001 2183 7880 1012 1996 3772 2001 2139 24759 6525 3468 1010 1996 4950 2147 1998 7497 2246 2004 2295 2009 2001 2915 1998 2448 2011 1037 5308 1997 2184 2095 2214 1005 1055 1012 2053 15226 2000 2184 2095 19457 1012 1045 2074 2481 1005 1056 2202 4902 1045 2288 2125 2026 6411 1010 2165 1996 1996 2160 1997 4205 4966 2041 1997 1996 4966 2447 1998 4711 2009 1999 1996 13044 1012 2672 2065 2017 2024 1037 6972 16248 5470 2023 3185 2089 3037 2017 1010 2138 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 7632 4364 1010 2023 2003 2026 2034 3319 1998 1045 2052 2018 2000 2031 3856 1996 5409 3185 2000 3319 1012 2004 1045 2069 3427 1019 2781 1997 2009 2021 3404 2033 2017 2071 2156 2023 3185 2001 2183 7880 1012 1996 3772 2001 2139 24759 6525 3468 1010 1996 4950 2147 1998 7497 2246 2004 2295 2009 2001 2915 1998 2448 2011 1037 5308 1997 2184 2095 2214 1005 1055 1012 2053 15226 2000 2184 2095 19457 1012 1045 2074 2481 1005 1056 2202 4902 1045 2288 2125 2026 6411 1010 2165 1996 1996 2160 1997 4205 4966 2041 1997 1996 4966 2447 1998 4711 2009 1999 1996 13044 1012 2672 2065 2017 2024 1037 6972 16248 5470 2023 3185 2089 3037 2017 1010 2138 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    }
   ],
   "source": [
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, \n",
    "                                                                  label_list, \n",
    "                                                                  MAX_SEQ_LENGTH, \n",
    "                                                                  tokenizer)\n",
    "\n",
    "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, \n",
    "                                                                 label_list, \n",
    "                                                                 MAX_SEQ_LENGTH, \n",
    "                                                                 tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
